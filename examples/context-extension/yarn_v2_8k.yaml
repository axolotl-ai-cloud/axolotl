base_model: /home/smishra/axolotl-kog/models/laneformer-tp-2B-nemov2phase1-step-620000
model_type: LlamaForCausalLM
tokenizer_type: AutoTokenizer
trust_remote_code: true

special_tokens:
  pad_token: "</s>" 

overrides_of_model_config:
  rope_scaling:
    type: yarn
    factor: 2.0
    original_max_position_embeddings: 4096
  max_position_embeddings: 8192
  pad_token_id: 2

sequence_len: 8192
context_parallel_size: 1   

datasets:
  - path: emozilla/pg19
    type: completion
    field: text
    split: train

dataset_prepared_path: ./prepared_data/dense_v2_yarn_8k
val_set_size: 0.05

load_in_8bit: false
load_in_4bit: false
adapter: null
gradient_checkpointing: true

num_epochs: 1
max_steps: 10000
micro_batch_size: 1
gradient_accumulation_steps: 8

learning_rate: 2.0e-05
lr_scheduler: cosine
warmup_steps: 1000
cosine_min_lr_ratio: 0.1

optimizer: adamw_torch_fused
weight_decay: 0.01
max_grad_norm: 0.3

flash_attention: true
sample_packing: false
pad_to_sequence_len: true
eval_sample_packing: false
train_on_inputs: true

bf16: auto
tf32: true
ddp: true

output_dir: ./outputs/dense_v2_yarn_8k
save_steps: 500
logging_steps: 10

include_tokens_per_second: true
deepspeed: /home/smishra/axolotl-kog/deepspeed_configs/zero3_bf16_cpuoffload_all.json
use_wandb: false
eos_token_id: 2
bos_token_id: 1
pad_token_id: 2
