{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "66c0fbf9f56f4e34a780237b197d0765": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d5e99f1ce9974ec18bb38985fa682512",
              "IPY_MODEL_329cab0a415e42a3898d6a44fe15998c",
              "IPY_MODEL_165b5f3ad43b43d39678b44f2dcff723",
              "IPY_MODEL_ccd91efce20b475587cf450de0789735"
            ],
            "layout": "IPY_MODEL_feb8c4a6d07842459213f2ad4a405b5d"
          }
        },
        "3f63123feba14853b8f422d7b1062123": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15ae13f194a14a529388e61b269c5576",
            "placeholder": "​",
            "style": "IPY_MODEL_760aa9e4dd664d2aa99ea8a70defb2a9",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "70fa05da2afe4632bd457e08d17607db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_47705c9475104659b2d2aa4c24271db9",
            "placeholder": "​",
            "style": "IPY_MODEL_fd8330ea47264dbf8596f351e0b60607",
            "value": ""
          }
        },
        "188dbf3df94f459bb111932fc35455cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_53915e5377f44c5fa5054b66b8178ba4",
            "style": "IPY_MODEL_eba8d5202f6e41899eb5133792137d9f",
            "value": true
          }
        },
        "2ea43dbac5144b69b94563fad6ec9d78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_bdf62ad6474844f0aca9a180f36e5bd7",
            "style": "IPY_MODEL_484dc1e12690484aa159d5dd20c6a793",
            "tooltip": ""
          }
        },
        "3b661f81f1d84f81b9c92abfe88424c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59af642cd092481e91db8ff40e3cb1ce",
            "placeholder": "​",
            "style": "IPY_MODEL_e698618794604c21aff9501440c05eef",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "feb8c4a6d07842459213f2ad4a405b5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "15ae13f194a14a529388e61b269c5576": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "760aa9e4dd664d2aa99ea8a70defb2a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47705c9475104659b2d2aa4c24271db9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd8330ea47264dbf8596f351e0b60607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53915e5377f44c5fa5054b66b8178ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eba8d5202f6e41899eb5133792137d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdf62ad6474844f0aca9a180f36e5bd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "484dc1e12690484aa159d5dd20c6a793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "59af642cd092481e91db8ff40e3cb1ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e698618794604c21aff9501440c05eef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9c1e2eeb3c542c4bd9793dee6cffa0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7615e4812804c90ab54151db742c8e6",
            "placeholder": "​",
            "style": "IPY_MODEL_d777ce5ab47047379f2d451bfaed0330",
            "value": "Connecting..."
          }
        },
        "c7615e4812804c90ab54151db742c8e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d777ce5ab47047379f2d451bfaed0330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5e99f1ce9974ec18bb38985fa682512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_114afd54a77a492b9df46ba1ff0f0c69",
            "placeholder": "​",
            "style": "IPY_MODEL_a1c0df3e05cd40a0a2b05c98475f8af9",
            "value": "Token is valid (permission: read)."
          }
        },
        "329cab0a415e42a3898d6a44fe15998c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4b9b5462a08401aa531803c24eef1bb",
            "placeholder": "​",
            "style": "IPY_MODEL_7c60e10951554fb9a5dd807c30c8c6ee",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "165b5f3ad43b43d39678b44f2dcff723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cf051bad649483caf2dcf4afee7940e",
            "placeholder": "​",
            "style": "IPY_MODEL_45285b02821640459f41f0ec74ea0426",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "ccd91efce20b475587cf450de0789735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb990f12a8eb48d6a6fcff64b3df2839",
            "placeholder": "​",
            "style": "IPY_MODEL_32d5bdf2e00b47eea4bdd4f3eed40d16",
            "value": "Login successful"
          }
        },
        "114afd54a77a492b9df46ba1ff0f0c69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1c0df3e05cd40a0a2b05c98475f8af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4b9b5462a08401aa531803c24eef1bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c60e10951554fb9a5dd807c30c8c6ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cf051bad649483caf2dcf4afee7940e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45285b02821640459f41f0ec74ea0426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb990f12a8eb48d6a6fcff64b3df2839": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32d5bdf2e00b47eea4bdd4f3eed40d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Setting up"
      ],
      "metadata": {
        "id": "-AL2CK1HB4f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Check so there is a gpu available, a T4(free tier) is enough to run this notebook\n",
        "assert (torch.cuda.is_available()==True)"
      ],
      "metadata": {
        "id": "-fZtnaAus24c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r axolotl #do a clean install"
      ],
      "metadata": {
        "id": "ycz6kSjd8ZIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e git+https://github.com/axolotl-ai-cloud/axolotl#egg=axolotl\n",
        "#!rm -r src\\\n",
        "#!git clone https://github.com/axolotl-ai-cloud/axolotl\n",
        "#!cd axolotl\n",
        "#!pip install flash-attn # comment out if using T4 GPU as flash attention currently doesn't support it\n",
        "!pip install deepspeed\n",
        "#!pip install mamba-ssm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMg8LT7nvW3c",
        "outputId": "8afdcf42-c4a5-4b25-edff-4125da36a709",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining axolotl from git+https://github.com/axolotl-ai-cloud/axolotl#egg=axolotl\n",
            "  Cloning https://github.com/axolotl-ai-cloud/axolotl to ./src/axolotl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/axolotl-ai-cloud/axolotl /content/src/axolotl\n",
            "  Resolved https://github.com/axolotl-ai-cloud/axolotl to commit 8c3a727f9d60ffd3af385f90bcc3fa3a56398fe1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe (from axolotl)\n",
            "  Cloning https://github.com/lm-sys/FastChat.git (to revision 27a05b04a35510afb1d767ae7e5990cbd278f8fe) to /tmp/pip-install-ka0djibm/fschat_91917ba1b49e4baab6383d5d2f033b5c\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lm-sys/FastChat.git /tmp/pip-install-ka0djibm/fschat_91917ba1b49e4baab6383d5d2f033b5c\n",
            "  Running command git rev-parse -q --verify 'sha^27a05b04a35510afb1d767ae7e5990cbd278f8fe'\n",
            "  Running command git fetch -q https://github.com/lm-sys/FastChat.git 27a05b04a35510afb1d767ae7e5990cbd278f8fe\n",
            "  Running command git checkout -q 27a05b04a35510afb1d767ae7e5990cbd278f8fe\n",
            "  Resolved https://github.com/lm-sys/FastChat.git to commit 27a05b04a35510afb1d767ae7e5990cbd278f8fe\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting trl@ git+https://github.com/huggingface/trl.git@31d02cfb795284591a084416b9dcb7bef5d08924 (from axolotl)\n",
            "  Cloning https://github.com/huggingface/trl.git (to revision 31d02cfb795284591a084416b9dcb7bef5d08924) to /tmp/pip-install-ka0djibm/trl_c2377960799b4ff6bd029d70842d0a6a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/trl.git /tmp/pip-install-ka0djibm/trl_c2377960799b4ff6bd029d70842d0a6a\n",
            "  Running command git rev-parse -q --verify 'sha^31d02cfb795284591a084416b9dcb7bef5d08924'\n",
            "  Running command git fetch -q https://github.com/huggingface/trl.git 31d02cfb795284591a084416b9dcb7bef5d08924\n",
            "  Running command git checkout -q 31d02cfb795284591a084416b9dcb7bef5d08924\n",
            "  Resolved https://github.com/huggingface/trl.git to commit 31d02cfb795284591a084416b9dcb7bef5d08924\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting packaging==23.2 (from axolotl)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting peft==0.13.2 (from axolotl)\n",
            "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting transformers==4.46.0 (from axolotl)\n",
            "  Downloading transformers-4.46.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.20.1 (from axolotl)\n",
            "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting bitsandbytes==0.44.1 (from axolotl)\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting accelerate==1.0.1 (from axolotl)\n",
            "  Downloading accelerate-1.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting datasets==3.0.1 (from axolotl)\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting pydantic==2.6.3 (from axolotl)\n",
            "  Downloading pydantic-2.6.3-py3-none-any.whl.metadata (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict (from axolotl)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting fire (from axolotl)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.10/dist-packages (from axolotl) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from axolotl) (2.32.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from axolotl) (0.2.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from axolotl) (0.18.5)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from axolotl) (0.8.0)\n",
            "Collecting optimum==1.16.2 (from axolotl)\n",
            "  Downloading optimum-1.16.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting hf_transfer (from axolotl)\n",
            "  Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting colorama (from axolotl)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from axolotl) (0.60.0)\n",
            "Requirement already satisfied: numpy<=2.0.1,>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from axolotl) (1.26.4)\n",
            "Collecting evaluate==0.4.1 (from axolotl)\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from axolotl) (1.13.1)\n",
            "Collecting scikit-learn==1.4.2 (from axolotl)\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting pynvml (from axolotl)\n",
            "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting art (from axolotl)\n",
            "  Downloading art-6.3-py3-none-any.whl.metadata (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio==3.50.2 (from axolotl)\n",
            "  Downloading gradio-3.50.2-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from axolotl) (2.17.0)\n",
            "Collecting python-dotenv==1.0.1 (from axolotl)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting triton>=2.3.0 (from axolotl)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting liger-kernel==0.3.0 (from axolotl)\n",
            "  Downloading liger_kernel-0.3.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting s3fs>=2024.5.0 (from axolotl)\n",
            "  Downloading s3fs-2024.10.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: gcsfs>=2024.5.0 in /usr/local/lib/python3.10/dist-packages (from axolotl) (2024.6.1)\n",
            "Collecting zstandard==0.22.0 (from axolotl)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.10/dist-packages (from axolotl) (1.7.19)\n",
            "Collecting lm_eval==0.4.4 (from axolotl)\n",
            "  Downloading lm_eval-0.4.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect==1.0.9 (from axolotl)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: immutabledict==4.2.0 in /usr/local/lib/python3.10/dist-packages (from axolotl) (4.2.0)\n",
            "Collecting antlr4-python3-runtime==4.13.2 (from axolotl)\n",
            "  Downloading antlr4_python3_runtime-4.13.2-py3-none-any.whl.metadata (304 bytes)\n",
            "Collecting torchao==0.5.0 (from axolotl)\n",
            "  Downloading torchao-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: torch==2.5.0+cu121 in /usr/local/lib/python3.10/dist-packages (from axolotl) (2.5.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==1.0.1->axolotl) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==1.0.1->axolotl) (0.24.7)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate==1.0.1->axolotl) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1->axolotl) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1->axolotl) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets==3.0.1->axolotl)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1->axolotl) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1->axolotl) (4.66.5)\n",
            "Collecting xxhash (from datasets==3.0.1->axolotl)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets==3.0.1->axolotl)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==3.0.1->axolotl) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==3.0.1->axolotl) (3.10.10)\n",
            "Collecting responses<0.19 (from evaluate==0.4.1->axolotl)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==3.50.2->axolotl)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl) (4.2.2)\n",
            "Collecting fastapi (from gradio==3.50.2->axolotl)\n",
            "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio==3.50.2->axolotl)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==0.6.1 (from gradio==3.50.2->axolotl)\n",
            "  Downloading gradio_client-0.6.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx (from gradio==3.50.2->axolotl)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio==3.50.2->axolotl)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio==3.50.2->axolotl)\n",
            "  Downloading orjson-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl) (10.4.0)\n",
            "Collecting pydub (from gradio==3.50.2->axolotl)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart (from gradio==3.50.2->axolotl)\n",
            "  Downloading python_multipart-0.0.16-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio==3.50.2->axolotl)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==3.50.2->axolotl)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio==3.50.2->axolotl)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect==1.0.9->axolotl) (1.16.0)\n",
            "Collecting jsonlines (from lm_eval==0.4.4->axolotl)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.4->axolotl) (2.10.1)\n",
            "Collecting pybind11>=2.6.2 (from lm_eval==0.4.4->axolotl)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pytablewriter (from lm_eval==0.4.4->axolotl)\n",
            "  Downloading pytablewriter-1.2.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting rouge-score>=0.0.4 (from lm_eval==0.4.4->axolotl)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu>=1.5.0 (from lm_eval==0.4.4->axolotl)\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlitedict (from lm_eval==0.4.4->axolotl)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tqdm-multiprocess (from lm_eval==0.4.4->axolotl)\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting word2number (from lm_eval==0.4.4->axolotl)\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from lm_eval==0.4.4->axolotl) (10.5.0)\n",
            "Collecting coloredlogs (from optimum==1.16.2->axolotl)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum==1.16.2->axolotl) (1.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.6.3->axolotl) (0.7.0)\n",
            "Collecting pydantic-core==2.16.3 (from pydantic==2.6.3->axolotl)\n",
            "  Downloading pydantic_core-2.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2->axolotl) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2->axolotl) (3.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0+cu121->axolotl) (3.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0->axolotl) (2024.9.11)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum==1.16.2->axolotl) (1.3.0)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs>=2024.5.0->axolotl) (4.4.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs>=2024.5.0->axolotl) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs>=2024.5.0->axolotl) (1.2.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from gcsfs>=2024.5.0->axolotl) (2.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->axolotl) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->axolotl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->axolotl) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->axolotl) (2024.8.30)\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs>=2024.5.0->axolotl)\n",
            "  Downloading aiobotocore-2.15.2-py3-none-any.whl.metadata (23 kB)\n",
            "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting s3fs>=2024.5.0 (from axolotl)\n",
            "  Downloading s3fs-2024.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "  Downloading s3fs-2024.6.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->axolotl) (2.5.0)\n",
            "Collecting markdown2[all] (from fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl)\n",
            "  Downloading markdown2-2.5.1-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting nh3 (from fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl)\n",
            "  Downloading nh3-0.2.18-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl) (3.0.48)\n",
            "Requirement already satisfied: rich>=10.0.0 in /usr/local/lib/python3.10/dist-packages (from fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl) (13.9.3)\n",
            "Collecting shortuuid (from fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl)\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting tiktoken (from fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->axolotl) (0.43.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl) (75.1.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl) (3.0.4)\n",
            "Collecting tyro>=0.5.11 (from trl@ git+https://github.com/huggingface/trl.git@31d02cfb795284591a084416b9dcb7bef5d08924->axolotl)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl) (4.3.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl) (2.17.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl) (1.3.3)\n",
            "Collecting botocore<1.35.37,>=1.35.16 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.5.0->axolotl)\n",
            "  Downloading botocore-1.35.36-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.5.0->axolotl) (1.16.0)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.5.0->axolotl)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1->axolotl) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1->axolotl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1->axolotl) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1->axolotl) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1->axolotl) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1->axolotl) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.0.1->axolotl) (4.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->axolotl) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->axolotl) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->axolotl) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->axolotl) (4.0.11)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs>=2024.5.0->axolotl) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs>=2024.5.0->axolotl) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs>=2024.5.0->axolotl) (4.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1->axolotl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.0.1->axolotl) (2024.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit>=3.0.0->fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl) (0.2.13)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.0.0->fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl) (2.18.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.4->axolotl) (3.8.1)\n",
            "Collecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.4->axolotl)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.4->axolotl) (0.9.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.4->axolotl) (4.9.4)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl@ git+https://github.com/huggingface/trl.git@31d02cfb795284591a084416b9dcb7bef5d08924->axolotl) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl@ git+https://github.com/huggingface/trl.git@31d02cfb795284591a084416b9dcb7bef5d08924->axolotl)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio==3.50.2->axolotl)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum==1.16.2->axolotl)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi->gradio==3.50.2->axolotl)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs>=2024.5.0->axolotl) (1.3.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs>=2024.5.0->axolotl) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs>=2024.5.0->axolotl) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs>=2024.5.0->axolotl) (2.7.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.50.2->axolotl) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx->gradio==3.50.2->axolotl)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.50.2->axolotl) (1.3.1)\n",
            "Collecting wavedrom (from markdown2[all]->fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl)\n",
            "  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting latex2mathml (from markdown2[all]->fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl)\n",
            "  Downloading latex2mathml-3.77.0-py3-none-any.whl.metadata (14 kB)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==3.0.1->axolotl)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting DataProperty<2,>=1.0.1 (from pytablewriter->lm_eval==0.4.4->axolotl)\n",
            "  Downloading DataProperty-1.0.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.4->axolotl)\n",
            "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.4->axolotl)\n",
            "  Downloading pathvalidate-3.2.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.4->axolotl)\n",
            "  Downloading tabledata-1.3.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.4->axolotl)\n",
            "  Downloading tcolorpy-0.1.6-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.4->axolotl)\n",
            "  Downloading typepy-1.3.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<1.35.37,>=1.35.16->aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.5.0->axolotl)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->axolotl) (5.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs>=2024.5.0->axolotl) (1.65.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs>=2024.5.0->axolotl) (1.24.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->gcsfs>=2024.5.0->axolotl) (1.6.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl) (0.1.2)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.4->axolotl) (5.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs>=2024.5.0->axolotl) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs>=2024.5.0->axolotl) (3.2.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio==3.50.2->axolotl) (1.2.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==3.0.1->axolotl) (0.2.0)\n",
            "Collecting svgwrite (from wavedrom->markdown2[all]->fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'transformers' candidate (version 4.46.0 at https://files.pythonhosted.org/packages/db/88/1ef8a624a33d7fe460a686b9e0194a7916320fc0d67d4e38e570beeac039/transformers-4.46.0-py3-none-any.whl (from https://pypi.org/simple/transformers/) (requires-python:>=3.8.0))\n",
            "Reason for being yanked: This version unfortunately does not work with 3.8 but we did not drop the support yet\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading liger_kernel-0.3.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_eval-0.4.4-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum-1.16.2-py3-none-any.whl (402 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.5/402.5 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.6.3-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.2/395.2 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchao-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.46.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3fs-2024.6.1-py3-none-any.whl (29 kB)\n",
            "Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading art-6.3-py3-none-any.whl (606 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.3/606.3 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiobotocore-2.15.2-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading orjson-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nh3-0.2.18-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.2/769.2 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.16-py3-none-any.whl (24 kB)\n",
            "Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading botocore-1.35.36-py3-none-any.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
            "Downloading pathvalidate-3.2.1-py3-none-any.whl (23 kB)\n",
            "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.6-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
            "Downloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown2-2.5.1-py2.py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect, fire, fschat, trl, rouge-score, sqlitedict, word2number, wavedrom\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=e3b466b679159a3255be333076dda83e6b3cf7dae77010360703926f5a9b10be\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=9a89a565f7e8e2da32dae72f205562cd749ca12f5ab3e62d17e7b2a068d0557c\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
            "  Building wheel for fschat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fschat: filename=fschat-0.2.36-py3-none-any.whl size=272080 sha256=25fd2fec8973218f4a534789d20f05ad4a87c1b1e57e22b564f2e6518ab66552\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/dc/55/8647f928ab3e6390d35d3bb898acca851918560726ecdfc42a\n",
            "  Building wheel for trl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for trl: filename=trl-0.12.0.dev0-py3-none-any.whl size=306869 sha256=397c0a470c7b057de1fba4f445616e1fd59d64a8f3fa521805122a6c93c58a9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/8e/5f/1290695a42f009746135f67c10885a5a51f91dba640cd9d687\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=80f368c015b25c099d13890793aa67dc59f43b936f60406e507323e04d76fddb\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=f0bf4606d905a44e06e566817132364f64f1de40344931058b8c69549e1beed9\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=1161ab2080f4600373d8ed277d601a49d3c67a2435dbca772c90ab58af9aa104\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
            "  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30052 sha256=5a0700acd83ae3433c8532069c9ac6058b4e8d52e8e6f030e0bf12b58555de45\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/52/8c/38b454b42f712f325e26f633287484c7dc1ad469e1580c5954\n",
            "Successfully built langdetect fire fschat trl rouge-score sqlitedict word2number wavedrom\n",
            "Installing collected packages: word2number, torchao, sqlitedict, pydub, nh3, antlr4-python3-runtime, addict, zstandard, xxhash, websockets, triton, tcolorpy, svgwrite, shtab, shortuuid, semantic-version, python-multipart, python-dotenv, pynvml, pydantic-core, pybind11, portalocker, pathvalidate, packaging, orjson, mbstrdecoder, markupsafe, markdown2, latex2mathml, langdetect, jsonlines, jmespath, humanfriendly, hf_transfer, h11, fire, ffmpy, dill, colorama, art, aioitertools, aiofiles, wavedrom, uvicorn, typepy, tqdm-multiprocess, tiktoken, starlette, scikit-learn, sacrebleu, rouge-score, responses, pydantic, multiprocess, httpcore, coloredlogs, botocore, tyro, tokenizers, httpx, fastapi, transformers, gradio-client, fschat, DataProperty, bitsandbytes, aiobotocore, accelerate, tabledata, s3fs, peft, liger-kernel, gradio, datasets, trl, pytablewriter, optimum, evaluate, lm_eval, axolotl\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.23.4\n",
            "    Uninstalling pydantic_core-2.23.4:\n",
            "      Successfully uninstalled pydantic_core-2.23.4\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.5.2\n",
            "    Uninstalling scikit-learn-1.5.2:\n",
            "      Successfully uninstalled scikit-learn-1.5.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.9.2\n",
            "    Uninstalling pydantic-2.9.2:\n",
            "      Successfully uninstalled pydantic-2.9.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.34.2\n",
            "    Uninstalling accelerate-0.34.2:\n",
            "      Successfully uninstalled accelerate-0.34.2\n",
            "  Running setup.py develop for axolotl\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 1.4.15 requires pydantic>=2.7.0, but you have pydantic 2.6.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed DataProperty-1.0.1 accelerate-1.0.1 addict-2.4.0 aiobotocore-2.15.2 aiofiles-23.2.1 aioitertools-0.12.0 antlr4-python3-runtime-4.13.2 art-6.3 axolotl-0.4.1 bitsandbytes-0.44.1 botocore-1.35.36 colorama-0.4.6 coloredlogs-15.0.1 datasets-3.0.1 dill-0.3.8 evaluate-0.4.1 fastapi-0.115.4 ffmpy-0.4.0 fire-0.7.0 fschat-0.2.36 gradio-3.50.2 gradio-client-0.6.1 h11-0.14.0 hf_transfer-0.1.8 httpcore-1.0.6 httpx-0.27.2 humanfriendly-10.0 jmespath-1.0.1 jsonlines-4.0.0 langdetect-1.0.9 latex2mathml-3.77.0 liger-kernel-0.3.0 lm_eval-0.4.4 markdown2-2.5.1 markupsafe-2.1.5 mbstrdecoder-1.1.3 multiprocess-0.70.16 nh3-0.2.18 optimum-1.16.2 orjson-3.10.10 packaging-23.2 pathvalidate-3.2.1 peft-0.13.2 portalocker-2.10.1 pybind11-2.13.6 pydantic-2.6.3 pydantic-core-2.16.3 pydub-0.25.1 pynvml-11.5.3 pytablewriter-1.2.0 python-dotenv-1.0.1 python-multipart-0.0.16 responses-0.18.0 rouge-score-0.1.2 s3fs-2024.6.1 sacrebleu-2.4.3 scikit-learn-1.4.2 semantic-version-2.10.0 shortuuid-1.0.13 shtab-1.7.1 sqlitedict-2.1.0 starlette-0.41.2 svgwrite-1.4.3 tabledata-1.3.3 tcolorpy-0.1.6 tiktoken-0.8.0 tokenizers-0.20.1 torchao-0.5.0 tqdm-multiprocess-0.0.11 transformers-4.46.0 triton-3.1.0 trl-0.12.0.dev0 typepy-1.3.2 tyro-0.8.14 uvicorn-0.32.0 wavedrom-2.0.3.post3 websockets-11.0.3 word2number-1.1 xxhash-3.5.0 zstandard-0.22.0\n",
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.15.3.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.1.0)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.6.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.66.5)\n",
            "Collecting nvidia-ml-py (from deepspeed)\n",
            "  Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.5)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.15.3-py3-none-any.whl size=1526208 sha256=27e8beb4849814480b1bda3f4e4db4d8a82beacec04c6c5609b70d56edae8e7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/c2/9f/37a2c813b8d64d7908793319cfdfa4f852754e177f20f0b858\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: nvidia-ml-py, ninja, hjson, deepspeed\n",
            "Successfully installed deepspeed-0.15.3 hjson-3.1.0 ninja-1.11.1.1 nvidia-ml-py-12.560.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hugging Face login"
      ],
      "metadata": {
        "id": "o1ELLM7zCAeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "Rc4HKiV8B_TT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "66c0fbf9f56f4e34a780237b197d0765",
            "3f63123feba14853b8f422d7b1062123",
            "70fa05da2afe4632bd457e08d17607db",
            "188dbf3df94f459bb111932fc35455cb",
            "2ea43dbac5144b69b94563fad6ec9d78",
            "3b661f81f1d84f81b9c92abfe88424c0",
            "feb8c4a6d07842459213f2ad4a405b5d",
            "15ae13f194a14a529388e61b269c5576",
            "760aa9e4dd664d2aa99ea8a70defb2a9",
            "47705c9475104659b2d2aa4c24271db9",
            "fd8330ea47264dbf8596f351e0b60607",
            "53915e5377f44c5fa5054b66b8178ba4",
            "eba8d5202f6e41899eb5133792137d9f",
            "bdf62ad6474844f0aca9a180f36e5bd7",
            "484dc1e12690484aa159d5dd20c6a793",
            "59af642cd092481e91db8ff40e3cb1ce",
            "e698618794604c21aff9501440c05eef",
            "c9c1e2eeb3c542c4bd9793dee6cffa0f",
            "c7615e4812804c90ab54151db742c8e6",
            "d777ce5ab47047379f2d451bfaed0330",
            "d5e99f1ce9974ec18bb38985fa682512",
            "329cab0a415e42a3898d6a44fe15998c",
            "165b5f3ad43b43d39678b44f2dcff723",
            "ccd91efce20b475587cf450de0789735",
            "114afd54a77a492b9df46ba1ff0f0c69",
            "a1c0df3e05cd40a0a2b05c98475f8af9",
            "d4b9b5462a08401aa531803c24eef1bb",
            "7c60e10951554fb9a5dd807c30c8c6ee",
            "2cf051bad649483caf2dcf4afee7940e",
            "45285b02821640459f41f0ec74ea0426",
            "bb990f12a8eb48d6a6fcff64b3df2839",
            "32d5bdf2e00b47eea4bdd4f3eed40d16"
          ]
        },
        "outputId": "9a39d2ac-7559-44b1-c6d9-dc7af8ff0a1d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66c0fbf9f56f4e34a780237b197d0765"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example configuration"
      ],
      "metadata": {
        "id": "n4MW2qH9ueGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "yaml_string = \"\"\"\n",
        "base_model: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n",
        "\n",
        "load_in_8bit: false\n",
        "load_in_4bit: true\n",
        "strict: false\n",
        "\n",
        "datasets:\n",
        "  - path: tatsu-lab/alpaca\n",
        "    type: alpaca\n",
        "dataset_prepared_path: last_run_prepared\n",
        "val_set_size: 0.05\n",
        "output_dir: ./outputs/lora-out\n",
        "\n",
        "sequence_len: 2048\n",
        "sample_packing: true\n",
        "eval_sample_packing: true\n",
        "pad_to_sequence_len: true\n",
        "\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "lora_r: 32\n",
        "lora_alpha: 16\n",
        "lora_dropout: 0.05\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "lora_modules_to_save:\n",
        "  - embed_tokens\n",
        "  - lm_head\n",
        "\n",
        "wandb_project:\n",
        "wandb_entity:\n",
        "wandb_watch:\n",
        "wandb_name:\n",
        "wandb_log_model:\n",
        "\n",
        "gradient_accumulation_steps: 2\n",
        "micro_batch_size: 1\n",
        "num_epochs: 1\n",
        "optimizer: paged_adamw_8bit\n",
        "lr_scheduler: cosine\n",
        "learning_rate: 2e-5\n",
        "\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: auto\n",
        "fp16:\n",
        "tf32: false\n",
        "\n",
        "gradient_checkpointing: true\n",
        "gradient_checkpointing_kwargs:\n",
        "  use_reentrant: false\n",
        "early_stopping_patience:\n",
        "resume_from_checkpoint:\n",
        "logging_steps: 1\n",
        "xformers_attention:\n",
        "flash_attention: false\n",
        "sdp_attention: true\n",
        "\n",
        "warmup_steps: 1\n",
        "max_steps: 25\n",
        "evals_per_epoch: 1\n",
        "eval_table_size:\n",
        "saves_per_epoch: 1\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.0\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: <|end_of_text|>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Convert the YAML string to a Python dictionary\n",
        "yaml_dict = yaml.safe_load(yaml_string)\n",
        "\n",
        "# Specify your file path\n",
        "file_path = 'qlora-4bit_colab.yaml'\n",
        "\n",
        "# Write the YAML file\n",
        "with open(file_path, 'w') as file:\n",
        "    yaml.dump(yaml_dict, file)"
      ],
      "metadata": {
        "id": "7s6woSfiv2UJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above we have a configuration file with base LLM model and datasets specified, amiong many other things. Axolotl can atuomatically detect whether the specifed datasets are on Hugguing Face repo or local machine.\n",
        "\n",
        "The Axolotl configuration options encompass model and dataset selection, data pre-processing, and training. Let's go through them line by line:\n",
        "\n",
        "*   \"base model\": String value, specifies the underlying pre-trained LLM that will be used for finetuning\n",
        "\n",
        "Next we have options for model weights quantization. Quantization allows for reduction in occupied memory on GPUs.\n",
        "\n",
        "*   \"load_in_8bit\": Boolean value, whether to quantize the model weights into 8-bit integer.\n",
        "\n",
        "*   \"load_in_4bit\": Boolean value, whether to quantize the model weights into 4-bit integer.\n",
        "\n",
        "*   \"strict\": Boolean value. If false, it allows for overidding established configuration options in the yaml file when executing in command-line interface.\n",
        "\n",
        "*   \"datasets\": a list of dicts that contain path and type of data sets as well as other optional configurations where datasets are concerned. Supports multiple datasets.\n",
        "\n",
        "*   \"val_set_size\": Either a float value less than one or an integer less than the total size of dataset. Sets the size of validation set from the whole dataset. If float, sets the proportion of the dataset assigned for validation. If integer, sets the direct size of validation set.\n",
        "\n",
        "*   \"output_dir\": String value. Path of trained model.\n",
        "\n",
        "For data preprocessing:\n",
        "\n",
        "*   \"sequence_len\": Integer. Specifies the maximum sequence length of the input. Typically 2048 or less.\n",
        "\n",
        "*   \"pad_to_sequence_len\": Boolean. Padding input to maximum sequence length.\n",
        "\n",
        "*   \"sample_packing\": Boolean. Specifies whether to use multi-packing with block diagonal attention.\n",
        "\n",
        "*   \"special_tokens\": Python dict, optional. Allows users to specify the additional special tokens to be ignored by the tokenizer.\n",
        "\n",
        "For LoRA configuration and its hyperparamters:\n",
        "\n",
        "*   \"adapter\": String. Either \"lora\" or \"qlora\", depending on user's choice.\n",
        "\n",
        "*   \"lora_model_dir\": String, Optional. Path to directory that contains LoRA model, if there is already a trained LoRA model the user would like to use.\n",
        "\n",
        "*   \"lora_r\": Integer. Refers to the rank of LoRA decomposition matrices. Higher value will reduce LoRA efficiency. Recommended to be set to 8.\n",
        "\n",
        "*   \"lora_alpha\": Integer. Scale the weight matrices by $\\frac{\\text{lora_alpha}}{\\text{lora_r}}$Recommended to be fixed at 16.\n",
        "\n",
        "*   \"lora_dropout\": Float that is 1 or less. The dropout probability of a lora layer.\n",
        "\n",
        "*   \"lora_target_linear\": Boolean. If true, lora will target all linear modules in the transformers architecture.\n",
        "\n",
        "*   \"lora_modules_to_save\": If you added new tokens to the tokenizer, you may need to save some LoRA modules because they need to know the new tokens.\n",
        "\n",
        "See [LoRA](https://arxiv.org/abs/2106.09685) for detailed explanation of LoRA implementation.\n",
        "\n",
        "For the training configurations:\n",
        "\n",
        "*   \"gradient_accumulation_steps\": Integer. The number of steps over which to accumulate gradient for batch training. E.g. if 2, backprop is performed every two steps.\n",
        "\n",
        "*   \"micro_batch_size\": Integer. Batch size per gpu / gradient_accumulation_steps\n",
        "\n",
        "*   \"num_epochs\": Integer. Number of epochs. One epoch is when training has looped over every batch in the whole data set once.\n",
        "\n",
        "*   \"optimizer\": The optimizer to use for the training.\n",
        "\n",
        "*   \"learning_rate\": The learning rate.\n",
        "\n",
        "*   \"lr_scheduler\": The learning rate scheduler to use for adjusting learning rate during training.\n",
        "\n",
        "*   \"train_on_inputs\": Boolean. Whether to ignore or include the user's prompt from the training labels.\n",
        "\n",
        "*   \"group_by_length\": Boolean. Whether to group similarly sized data to minimize padding.\n",
        "\n",
        "*   \"bf16\": Either \"auto\", \"true\", or \"false\". Whether to use CUDA bf16 floating point format. If set to \"auto\", will automatically apply bf16 should the gpu supports it.\n",
        "\n",
        "*   \"fp16\": Optional. Specifies whether to use CUDA fp16. Automatically set to true if \"bf16\" is set to true. Otherwise false.\n",
        "\n",
        "*   \"tf32\": Boolean. Whether to use CUDA tf32. Will override bf16.\n",
        "\n",
        "*   \"gradient_checkpointing\": Boolean. Whether to use gradient checkpointing https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-checkpointing\n",
        "\n",
        "*   \"gradient_checkpointing_kwargs\": Python Dict. Fed into the trainer.\n",
        "\n",
        "*   \"logging_steps\": Integer. Log training information over every specified number of steps.\n",
        "\n",
        "*   \"flash_attention\": Boolean. Whether to use the [flash attention](https://github.com/Dao-AILab/flash-attention) mechanism.\n",
        "\n",
        "*   \"sdp_attention\": Boolean. Whether to use the Scaled Dot Product attention mechanism (the attention mechanism in the [original implementation](https://arxiv.org/abs/1706.03762) of transformers.)\n",
        "\n",
        "*   \"warmup_steps\": Integer. The number of pre-training steps where a very low learning rate is used.\n",
        "\n",
        "*   \"evals_per_epoch\": Integer. Number of evaluations to be performed within one training epoch.\n",
        "\n",
        "*   \"saves_per_epoch\": Integer. Number of times the model is saved in one training epoch.\n",
        "\n",
        "*   \"weight_decay\": Positive Float. Sets the \"strength\" of weight decay (i.e. setting the coeeficient of L2 regularization)\n",
        "\n"
      ],
      "metadata": {
        "id": "g-AtstmJ4H_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above is but a snippet aiming to get users familiarized with the types of streamlined configuration options axolotl provides. For a full list of configuration options, see [here](https://github.com/axolotl-ai-cloud/axolotl/blob/main/docs/config.qmd)\n"
      ],
      "metadata": {
        "id": "LS-qnfT36k4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "Y_yN-xB9IAAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch -m axolotl.cli.train \"qlora-4bit_colab.yaml\""
      ],
      "metadata": {
        "id": "zx8BwCeX4-FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac752533-0d5d-41f0-a520-6a3eb2017586"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-10-29 16:39:28.662165: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-29 16:39:28.681418: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-29 16:39:28.687227: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-29 16:39:28.701131: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-29 16:39:30.382807: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/src/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:29: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, forward_function, hidden_states, *args):\n",
            "/content/src/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:40: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dY):\n",
            "[2024-10-29 16:39:36,686] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-10-29 16:39:36,832] [INFO] [root.spawn:60] [PID:5495] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmpugg2w8q2/test.c -o /tmp/tmpugg2w8q2/test.o\n",
            "[2024-10-29 16:39:36,856] [INFO] [root.spawn:60] [PID:5495] x86_64-linux-gnu-gcc /tmp/tmpugg2w8q2/test.o -laio -o /tmp/tmpugg2w8q2/a.out\n",
            "[2024-10-29 16:39:38,286] [INFO] [root.spawn:60] [PID:5495] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmpe3w19m09/test.c -o /tmp/tmpe3w19m09/test.o\n",
            "[2024-10-29 16:39:38,318] [INFO] [root.spawn:60] [PID:5495] x86_64-linux-gnu-gcc /tmp/tmpe3w19m09/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmpe3w19m09/a.out\n",
            "[2024-10-29 16:39:38,385] [INFO] [root.spawn:60] [PID:5495] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmpjqo5z4h0/test.c -o /tmp/tmpjqo5z4h0/test.o\n",
            "[2024-10-29 16:39:38,410] [INFO] [root.spawn:60] [PID:5495] x86_64-linux-gnu-gcc /tmp/tmpjqo5z4h0/test.o -laio -o /tmp/tmpjqo5z4h0/a.out\n",
            "/content/src/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
            "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "\u001b[33m[2024-10-29 16:39:41,238] [WARNING] [axolotl.utils.config.models.input.check_sample_packing_w_sdpa_bf16:1440] [PID:5495] [RANK:0] sample_packing & torch sdpa with bf16 is unsupported may results in 0.0 loss. This may work on H100s.\u001b[39m\n",
            "[2024-10-29 16:39:41,239] [DEBUG] [axolotl.normalize_config:83] [PID:5495] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
            "config.json: 100% 560/560 [00:00<00:00, 3.33MB/s]\n",
            "[2024-10-29 16:39:41,445] [INFO] [axolotl.normalize_config:207] [PID:5495] [RANK:0] GPU memory usage baseline: 0.000GB (+0.002GB cache, +0.352GB misc)\u001b[39m\n",
            "\n",
            "     #@@ #@@      @@# @@#\n",
            "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
            "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
            "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
            "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
            "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
            "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
            "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
            "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
            "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
            "    @@@@  @@@@@@@@@@@@@@@@\n",
            "\n",
            "tokenizer_config.json: 100% 776/776 [00:00<00:00, 4.50MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 17.6MB/s]\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 9.71MB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 2.64MB/s]\n",
            "[2024-10-29 16:39:42,906] [DEBUG] [axolotl.load_tokenizer:293] [PID:5495] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
            "[2024-10-29 16:39:42,906] [DEBUG] [axolotl.load_tokenizer:294] [PID:5495] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2024-10-29 16:39:42,906] [DEBUG] [axolotl.load_tokenizer:295] [PID:5495] [RANK:0] PAD: 32000 / <|end_of_text|>\u001b[39m\n",
            "[2024-10-29 16:39:42,906] [DEBUG] [axolotl.load_tokenizer:296] [PID:5495] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2024-10-29 16:39:42,907] [INFO] [axolotl.load_tokenizer:310] [PID:5495] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2024-10-29 16:39:42,907] [INFO] [axolotl.load_tokenized_prepared_datasets:210] [PID:5495] [RANK:0] Unable to find prepared dataset in last_run_prepared/a4545c7d491e2f110512ac3bacac84a6\u001b[39m\n",
            "[2024-10-29 16:39:42,907] [INFO] [axolotl.load_tokenized_prepared_datasets:211] [PID:5495] [RANK:0] Loading raw datasets...\u001b[39m\n",
            "\u001b[33m[2024-10-29 16:39:42,907] [WARNING] [axolotl.load_tokenized_prepared_datasets:213] [PID:5495] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n",
            "[2024-10-29 16:39:42,907] [INFO] [axolotl.load_tokenized_prepared_datasets:220] [PID:5495] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n",
            "README.md: 100% 7.47k/7.47k [00:00<00:00, 33.4MB/s]\n",
            "(…)-00000-of-00001-a09b74b3ef9c3b56.parquet: 100% 24.2M/24.2M [00:00<00:00, 163MB/s]\n",
            "Generating train split: 100% 52002/52002 [00:00<00:00, 230598.16 examples/s]\n",
            "[2024-10-29 16:39:46,495] [INFO] [axolotl.get_dataset_wrapper:588] [PID:5495] [RANK:0] Loading dataset with base_type: alpaca and prompt_style: None\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):   0% 244/52002 [00:00<01:27, 590.96 examples/s]\u001b[33m[2024-10-29 16:39:47,168] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):   4% 2150/52002 [00:01<00:34, 1441.56 examples/s]\u001b[33m[2024-10-29 16:39:48,229] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):   5% 2560/52002 [00:01<00:29, 1690.89 examples/s]\u001b[33m[2024-10-29 16:39:48,348] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):   6% 3330/52002 [00:02<00:33, 1469.53 examples/s]\u001b[33m[2024-10-29 16:39:48,951] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):   8% 4419/52002 [00:03<00:51, 929.75 examples/s]\u001b[33m[2024-10-29 16:39:50,173] [WARNING] [axolotl._tokenize:72] [PID:5634] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  18% 9366/52002 [00:08<00:48, 883.84 examples/s]\u001b[33m[2024-10-29 16:39:55,541] [WARNING] [axolotl._tokenize:72] [PID:5634] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  22% 11441/52002 [00:10<00:21, 1881.20 examples/s]\u001b[33m[2024-10-29 16:39:56,877] [WARNING] [axolotl._tokenize:72] [PID:5634] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  25% 12831/52002 [00:11<00:21, 1840.47 examples/s]\u001b[33m[2024-10-29 16:39:57,711] [WARNING] [axolotl._tokenize:72] [PID:5634] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  29% 15335/52002 [00:12<00:18, 2023.09 examples/s]\u001b[33m[2024-10-29 16:39:58,980] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  34% 17931/52002 [00:13<00:17, 1900.16 examples/s]\u001b[33m[2024-10-29 16:40:00,469] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  38% 19623/52002 [00:14<00:16, 1948.95 examples/s]\u001b[33m[2024-10-29 16:40:01,358] [WARNING] [axolotl._tokenize:72] [PID:5634] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  43% 22400/52002 [00:16<00:16, 1761.39 examples/s]\u001b[33m[2024-10-29 16:40:02,889] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  43% 22584/52002 [00:16<00:16, 1777.65 examples/s]\u001b[33m[2024-10-29 16:40:02,994] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  44% 22768/52002 [00:16<00:16, 1789.10 examples/s]\u001b[33m[2024-10-29 16:40:03,116] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  56% 29085/52002 [00:20<00:20, 1140.81 examples/s]\u001b[33m[2024-10-29 16:40:07,157] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  58% 30024/52002 [00:21<00:25, 865.46 examples/s] \u001b[33m[2024-10-29 16:40:08,062] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  70% 36494/52002 [00:27<00:09, 1574.55 examples/s]\u001b[33m[2024-10-29 16:40:14,418] [WARNING] [axolotl._tokenize:72] [PID:5634] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  75% 39024/52002 [00:29<00:07, 1830.45 examples/s]\u001b[33m[2024-10-29 16:40:15,810] [WARNING] [axolotl._tokenize:72] [PID:5634] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  78% 40499/52002 [00:30<00:06, 1707.09 examples/s]\u001b[33m[2024-10-29 16:40:16,634] [WARNING] [axolotl._tokenize:72] [PID:5634] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  80% 41511/52002 [00:30<00:05, 1938.39 examples/s]\u001b[33m[2024-10-29 16:40:17,176] [WARNING] [axolotl._tokenize:72] [PID:5634] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  89% 46483/52002 [00:33<00:03, 1518.19 examples/s]\u001b[33m[2024-10-29 16:40:19,994] [WARNING] [axolotl._tokenize:72] [PID:5634] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "\u001b[33m[2024-10-29 16:40:20,054] [WARNING] [axolotl._tokenize:72] [PID:5634] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  90% 46702/52002 [00:33<00:03, 1681.02 examples/s]\u001b[33m[2024-10-29 16:40:20,133] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  93% 48346/52002 [00:34<00:02, 1636.78 examples/s]\u001b[33m[2024-10-29 16:40:21,053] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  93% 48553/52002 [00:34<00:01, 1745.99 examples/s]\u001b[33m[2024-10-29 16:40:21,179] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  94% 48757/52002 [00:34<00:01, 1818.65 examples/s]\u001b[33m[2024-10-29 16:40:21,272] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  96% 49835/52002 [00:35<00:01, 1731.46 examples/s]\u001b[33m[2024-10-29 16:40:21,872] [WARNING] [axolotl._tokenize:72] [PID:5633] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2):  98% 50822/52002 [00:35<00:00, 1811.99 examples/s]\u001b[33m[2024-10-29 16:40:22,487] [WARNING] [axolotl._tokenize:72] [PID:5634] [RANK:0] Empty text requested for tokenization.\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2): 100% 52002/52002 [00:36<00:00, 1418.09 examples/s]\n",
            "Dropping Long Sequences (num_proc=2): 100% 52002/52002 [00:11<00:00, 4483.16 examples/s]\n",
            "Drop Samples with Zero Trainable Tokens (num_proc=2): 100% 52002/52002 [00:11<00:00, 4354.56 examples/s]\n",
            "Add position_id column (Sample Packing) (num_proc=2): 100% 51974/51974 [00:10<00:00, 4923.49 examples/s]\n",
            "[2024-10-29 16:40:58,098] [INFO] [axolotl.load_tokenized_prepared_datasets:467] [PID:5495] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/a4545c7d491e2f110512ac3bacac84a6\u001b[39m\n",
            "Saving the dataset (1/1 shards): 100% 51974/51974 [00:00<00:00, 143897.86 examples/s]\n",
            "[2024-10-29 16:40:58,508] [DEBUG] [axolotl.calculate_total_num_steps:320] [PID:5495] [RANK:0] total_num_tokens: 337_565\u001b[39m\n",
            "[2024-10-29 16:40:58,548] [DEBUG] [axolotl.calculate_total_num_steps:338] [PID:5495] [RANK:0] `total_supervised_tokens: 174_432`\u001b[39m\n",
            "[2024-10-29 16:41:05,559] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:197] [PID:5495] [RANK:0] gather_len_batches: [172]\u001b[39m\n",
            "[2024-10-29 16:41:05,560] [DEBUG] [axolotl.calculate_total_num_steps:390] [PID:5495] [RANK:0] data_loader_len: 85\u001b[39m\n",
            "[2024-10-29 16:41:05,560] [INFO] [axolotl.calc_sample_packing_eff_est:396] [PID:5495] [RANK:0] sample_packing_eff_est across ranks: [0.9582945357921512]\u001b[39m\n",
            "[2024-10-29 16:41:05,560] [DEBUG] [axolotl.calculate_total_num_steps:408] [PID:5495] [RANK:0] sample_packing_eff_est: None\u001b[39m\n",
            "[2024-10-29 16:41:05,560] [DEBUG] [axolotl.calculate_total_num_steps:416] [PID:5495] [RANK:0] total_num_steps: 85\u001b[39m\n",
            "[2024-10-29 16:41:05,613] [DEBUG] [axolotl.calculate_total_num_steps:320] [PID:5495] [RANK:0] total_num_tokens: 6_323_022\u001b[39m\n",
            "[2024-10-29 16:41:06,056] [DEBUG] [axolotl.calculate_total_num_steps:338] [PID:5495] [RANK:0] `total_supervised_tokens: 3_232_359`\u001b[39m\n",
            "[2024-10-29 16:41:06,100] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:197] [PID:5495] [RANK:0] gather_len_batches: [3213]\u001b[39m\n",
            "[2024-10-29 16:41:06,100] [DEBUG] [axolotl.calculate_total_num_steps:390] [PID:5495] [RANK:0] data_loader_len: 1603\u001b[39m\n",
            "[2024-10-29 16:41:06,100] [INFO] [axolotl.calc_sample_packing_eff_est:396] [PID:5495] [RANK:0] sample_packing_eff_est across ranks: [0.960912880777311]\u001b[39m\n",
            "[2024-10-29 16:41:06,100] [DEBUG] [axolotl.calculate_total_num_steps:408] [PID:5495] [RANK:0] sample_packing_eff_est: 0.97\u001b[39m\n",
            "[2024-10-29 16:41:06,100] [DEBUG] [axolotl.calculate_total_num_steps:416] [PID:5495] [RANK:0] total_num_steps: 1603\u001b[39m\n",
            "[2024-10-29 16:41:06,100] [INFO] [axolotl.prepare_dataset:128] [PID:5495] [RANK:0] Maximum number of steps set at 25\u001b[39m\n",
            "[2024-10-29 16:41:06,106] [DEBUG] [axolotl.train.train:66] [PID:5495] [RANK:0] loading tokenizer... TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\u001b[39m\n",
            "[2024-10-29 16:41:06,424] [DEBUG] [axolotl.load_tokenizer:293] [PID:5495] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
            "[2024-10-29 16:41:06,424] [DEBUG] [axolotl.load_tokenizer:294] [PID:5495] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2024-10-29 16:41:06,424] [DEBUG] [axolotl.load_tokenizer:295] [PID:5495] [RANK:0] PAD: 32000 / <|end_of_text|>\u001b[39m\n",
            "[2024-10-29 16:41:06,424] [DEBUG] [axolotl.load_tokenizer:296] [PID:5495] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2024-10-29 16:41:06,424] [INFO] [axolotl.load_tokenizer:310] [PID:5495] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2024-10-29 16:41:06,424] [DEBUG] [axolotl.train.train:98] [PID:5495] [RANK:0] loading model and peft_config...\u001b[39m\n",
            "[2024-10-29 16:41:06,517] [INFO] [axolotl.patch_llama_derived_model:516] [PID:5495] [RANK:0] patching llama _prepare_4d_causal_attention_mask*\u001b[39m\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
            "model.safetensors: 100% 4.40G/4.40G [01:44<00:00, 42.3MB/s]\n",
            "generation_config.json: 100% 129/129 [00:00<00:00, 786kB/s]\n",
            "[2024-10-29 16:43:13,832] [INFO] [axolotl.load_model:1059] [PID:5495] [RANK:0] GPU memory usage after model load: 0.841GB (+0.043GB cache, +0.368GB misc)\u001b[39m\n",
            "[2024-10-29 16:43:13,863] [INFO] [axolotl.load_model:1087] [PID:5495] [RANK:0] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
            "[2024-10-29 16:43:13,866] [INFO] [axolotl.load_lora:1284] [PID:5495] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
            "trainable params: 156,307,456 || all params: 1,256,359,936 || trainable%: 12.4413\n",
            "[2024-10-29 16:43:14,332] [INFO] [axolotl.load_model:1156] [PID:5495] [RANK:0] GPU memory usage after adapters: 1.426GB (+0.047GB cache, +0.368GB misc)\u001b[39m\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/content/src/axolotl/src/axolotl/core/trainer_builder.py:416: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AxolotlTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*_args, **kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "[2024-10-29 16:43:15,639] [INFO] [axolotl.train.train:141] [PID:5495] [RANK:0] Pre-saving adapter config to ./outputs/lora-out\u001b[39m\n",
            "[2024-10-29 16:43:15,680] [INFO] [axolotl.train.train:178] [PID:5495] [RANK:0] Starting trainer...\u001b[39m\n",
            "[2024-10-29 16:43:16,155] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:197] [PID:5495] [RANK:0] gather_len_batches: [3212]\u001b[39m\n",
            "  0% 0/25 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 5.95, 'grad_norm': 13.218790054321289, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
            "  4% 1/25 [00:17<06:52, 17.19s/it][2024-10-29 16:43:49,321] [INFO] [axolotl.callbacks.on_step_end:128] [PID:5495] [RANK:0] GPU memory usage while training: 1.104GB (+3.449GB cache, +0.761GB misc)\u001b[39m\n",
            "{'loss': 5.9255, 'grad_norm': 9.222084045410156, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
            "{'loss': 5.8006, 'grad_norm': 7.864278316497803, 'learning_rate': 1.9906859460363307e-05, 'epoch': 0.0}\n",
            "{'loss': 6.2918, 'grad_norm': 8.372536659240723, 'learning_rate': 1.9629172873477995e-05, 'epoch': 0.0}\n",
            "{'loss': 5.582, 'grad_norm': 9.291400909423828, 'learning_rate': 1.917211301505453e-05, 'epoch': 0.0}\n",
            "{'loss': 6.3011, 'grad_norm': 10.545504570007324, 'learning_rate': 1.8544194045464888e-05, 'epoch': 0.0}\n",
            "{'loss': 5.816, 'grad_norm': 8.82724666595459, 'learning_rate': 1.77571129070442e-05, 'epoch': 0.0}\n",
            "{'loss': 6.311, 'grad_norm': 7.805800914764404, 'learning_rate': 1.6825531432186545e-05, 'epoch': 0.0}\n",
            "{'loss': 6.766, 'grad_norm': 55.99869155883789, 'learning_rate': 1.5766803221148676e-05, 'epoch': 0.01}\n",
            "{'loss': 5.9744, 'grad_norm': 8.171456336975098, 'learning_rate': 1.4600650377311523e-05, 'epoch': 0.01}\n",
            "{'loss': 6.1459, 'grad_norm': 7.656591415405273, 'learning_rate': 1.3348796121709862e-05, 'epoch': 0.01}\n",
            "{'loss': 6.0108, 'grad_norm': 9.433976173400879, 'learning_rate': 1.2034560130526341e-05, 'epoch': 0.01}\n",
            "{'loss': 5.5321, 'grad_norm': 8.8348970413208, 'learning_rate': 1.0682424133646712e-05, 'epoch': 0.01}\n",
            "{'loss': 5.6739, 'grad_norm': 8.872214317321777, 'learning_rate': 9.317575866353293e-06, 'epoch': 0.01}\n",
            "{'loss': 6.3211, 'grad_norm': 6.489755630493164, 'learning_rate': 7.965439869473664e-06, 'epoch': 0.01}\n",
            "{'loss': 5.9914, 'grad_norm': 6.772642612457275, 'learning_rate': 6.651203878290139e-06, 'epoch': 0.01}\n",
            "{'loss': 5.6038, 'grad_norm': 7.098328590393066, 'learning_rate': 5.399349622688479e-06, 'epoch': 0.01}\n",
            "{'loss': 6.1074, 'grad_norm': 19.154918670654297, 'learning_rate': 4.2331967788513295e-06, 'epoch': 0.01}\n",
            "{'loss': 5.817, 'grad_norm': 7.580533504486084, 'learning_rate': 3.174468567813461e-06, 'epoch': 0.01}\n",
            "{'loss': 5.9095, 'grad_norm': 9.669846534729004, 'learning_rate': 2.2428870929558012e-06, 'epoch': 0.01}\n",
            "{'loss': 5.4183, 'grad_norm': 6.634291172027588, 'learning_rate': 1.4558059545351144e-06, 'epoch': 0.01}\n",
            "{'loss': 5.9304, 'grad_norm': 5.1697564125061035, 'learning_rate': 8.278869849454718e-07, 'epoch': 0.01}\n",
            "{'loss': 6.0043, 'grad_norm': 6.890308380126953, 'learning_rate': 3.708271265220087e-07, 'epoch': 0.01}\n",
            "{'loss': 5.78, 'grad_norm': 7.844511032104492, 'learning_rate': 9.314053963669245e-08, 'epoch': 0.01}\n",
            "{'loss': 5.6712, 'grad_norm': 7.766139507293701, 'learning_rate': 0.0, 'epoch': 0.02}\n",
            "100% 25/25 [06:58<00:00, 16.75s/it]/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "[2024-10-29 16:50:29,067] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:197] [PID:5495] [RANK:0] gather_len_batches: [173]\u001b[39m\n",
            "\n",
            "  0% 0/172 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/172 [00:02<03:42,  1.31s/it]\u001b[A\n",
            "  2% 3/172 [00:05<05:17,  1.88s/it]\u001b[A\n",
            "  2% 4/172 [00:07<06:02,  2.16s/it]\u001b[A\n",
            "  3% 5/172 [00:10<06:29,  2.33s/it]\u001b[A\n",
            "  3% 6/172 [00:13<06:45,  2.44s/it]\u001b[A\n",
            "  4% 7/172 [00:15<06:56,  2.52s/it]\u001b[A\n",
            "  5% 8/172 [00:18<07:03,  2.59s/it]\u001b[A\n",
            "  5% 9/172 [00:21<07:09,  2.63s/it]\u001b[A\n",
            "  6% 10/172 [00:24<07:13,  2.67s/it]\u001b[A\n",
            "  6% 11/172 [00:26<07:16,  2.71s/it]\u001b[A\n",
            "  7% 12/172 [00:29<07:17,  2.73s/it]\u001b[A\n",
            "  8% 13/172 [00:32<07:16,  2.75s/it]\u001b[A\n",
            "  8% 14/172 [00:35<07:14,  2.75s/it]\u001b[A\n",
            "  9% 15/172 [00:38<07:11,  2.75s/it]\u001b[A\n",
            "  9% 16/172 [00:40<07:08,  2.74s/it]\u001b[A\n",
            " 10% 17/172 [00:43<07:04,  2.74s/it]\u001b[A\n",
            " 10% 18/172 [00:46<06:59,  2.73s/it]\u001b[A\n",
            " 11% 19/172 [00:48<06:55,  2.72s/it]\u001b[A\n",
            " 12% 20/172 [00:51<06:50,  2.70s/it]\u001b[A\n",
            " 12% 21/172 [00:54<06:46,  2.69s/it]\u001b[A\n",
            " 13% 22/172 [00:56<06:42,  2.68s/it]\u001b[A\n",
            " 13% 23/172 [00:59<06:37,  2.67s/it]\u001b[A\n",
            " 14% 24/172 [01:02<06:33,  2.66s/it]\u001b[A\n",
            " 15% 25/172 [01:04<06:30,  2.65s/it]\u001b[A\n",
            " 15% 26/172 [01:07<06:26,  2.65s/it]\u001b[A\n",
            " 16% 27/172 [01:10<06:22,  2.64s/it]\u001b[A\n",
            " 16% 28/172 [01:12<06:19,  2.63s/it]\u001b[A\n",
            " 17% 29/172 [01:15<06:16,  2.63s/it]\u001b[A\n",
            " 17% 30/172 [01:17<06:12,  2.63s/it]\u001b[A\n",
            " 18% 31/172 [01:20<06:10,  2.63s/it]\u001b[A\n",
            " 19% 32/172 [01:23<06:07,  2.62s/it]\u001b[A\n",
            " 19% 33/172 [01:25<06:05,  2.63s/it]\u001b[A\n",
            " 20% 34/172 [01:28<06:02,  2.63s/it]\u001b[A\n",
            " 20% 35/172 [01:31<06:00,  2.63s/it]\u001b[A\n",
            " 21% 36/172 [01:33<05:58,  2.63s/it]\u001b[A\n",
            " 22% 37/172 [01:36<05:56,  2.64s/it]\u001b[A\n",
            " 22% 38/172 [01:38<05:54,  2.64s/it]\u001b[A\n",
            " 23% 39/172 [01:41<05:52,  2.65s/it]\u001b[A\n",
            " 23% 40/172 [01:44<05:51,  2.66s/it]\u001b[A\n",
            " 24% 41/172 [01:47<05:49,  2.67s/it]\u001b[A\n",
            " 24% 42/172 [01:49<05:47,  2.67s/it]\u001b[A\n",
            " 25% 43/172 [01:52<05:45,  2.68s/it]\u001b[A\n",
            " 26% 44/172 [01:55<05:42,  2.68s/it]\u001b[A\n",
            " 26% 45/172 [01:57<05:40,  2.68s/it]\u001b[A\n",
            " 27% 46/172 [02:00<05:38,  2.69s/it]\u001b[A\n",
            " 27% 47/172 [02:03<05:36,  2.69s/it]\u001b[A\n",
            " 28% 48/172 [02:05<05:33,  2.69s/it]\u001b[A\n",
            " 28% 49/172 [02:08<05:30,  2.69s/it]\u001b[A\n",
            " 29% 50/172 [02:11<05:28,  2.69s/it]\u001b[A\n",
            " 30% 51/172 [02:13<05:25,  2.69s/it]\u001b[A\n",
            " 30% 52/172 [02:16<05:22,  2.69s/it]\u001b[A\n",
            " 31% 53/172 [02:19<05:19,  2.68s/it]\u001b[A\n",
            " 31% 54/172 [02:21<05:16,  2.68s/it]\u001b[A\n",
            " 32% 55/172 [02:24<05:13,  2.68s/it]\u001b[A\n",
            " 33% 56/172 [02:27<05:10,  2.67s/it]\u001b[A\n",
            " 33% 57/172 [02:29<05:07,  2.67s/it]\u001b[A\n",
            " 34% 58/172 [02:32<05:04,  2.67s/it]\u001b[A\n",
            " 34% 59/172 [02:35<05:01,  2.67s/it]\u001b[A\n",
            " 35% 60/172 [02:37<04:58,  2.66s/it]\u001b[A\n",
            " 35% 61/172 [02:40<04:55,  2.66s/it]\u001b[A\n",
            " 36% 62/172 [02:43<04:52,  2.66s/it]\u001b[A\n",
            " 37% 63/172 [02:45<04:49,  2.66s/it]\u001b[A\n",
            " 37% 64/172 [02:48<04:46,  2.65s/it]\u001b[A\n",
            " 38% 65/172 [02:51<04:43,  2.65s/it]\u001b[A\n",
            " 38% 66/172 [02:53<04:40,  2.65s/it]\u001b[A\n",
            " 39% 67/172 [02:56<04:38,  2.65s/it]\u001b[A\n",
            " 40% 68/172 [02:59<04:35,  2.65s/it]\u001b[A\n",
            " 40% 69/172 [03:01<04:33,  2.65s/it]\u001b[A\n",
            " 41% 70/172 [03:04<04:30,  2.65s/it]\u001b[A\n",
            " 41% 71/172 [03:07<04:28,  2.66s/it]\u001b[A\n",
            " 42% 72/172 [03:09<04:25,  2.66s/it]\u001b[A\n",
            " 42% 73/172 [03:12<04:23,  2.66s/it]\u001b[A\n",
            " 43% 74/172 [03:15<04:20,  2.66s/it]\u001b[A\n",
            " 44% 75/172 [03:17<04:17,  2.66s/it]\u001b[A\n",
            " 44% 76/172 [03:20<04:15,  2.66s/it]\u001b[A\n",
            " 45% 77/172 [03:23<04:13,  2.67s/it]\u001b[A\n",
            " 45% 78/172 [03:25<04:10,  2.67s/it]\u001b[A\n",
            " 46% 79/172 [03:28<04:08,  2.67s/it]\u001b[A\n",
            " 47% 80/172 [03:31<04:05,  2.67s/it]\u001b[A\n",
            " 47% 81/172 [03:33<04:02,  2.67s/it]\u001b[A\n",
            " 48% 82/172 [03:36<03:59,  2.67s/it]\u001b[A\n",
            " 48% 83/172 [03:39<03:57,  2.67s/it]\u001b[A\n",
            " 49% 84/172 [03:41<03:54,  2.67s/it]\u001b[A\n",
            " 49% 85/172 [03:44<03:52,  2.67s/it]\u001b[A\n",
            " 50% 86/172 [03:47<03:49,  2.67s/it]\u001b[A\n",
            " 51% 87/172 [03:49<03:47,  2.67s/it]\u001b[A\n",
            " 51% 88/172 [03:52<03:44,  2.67s/it]\u001b[A\n",
            " 52% 89/172 [03:55<03:41,  2.67s/it]\u001b[A\n",
            " 52% 90/172 [03:57<03:38,  2.67s/it]\u001b[A\n",
            " 53% 91/172 [04:00<03:36,  2.67s/it]\u001b[A\n",
            " 53% 92/172 [04:03<03:33,  2.67s/it]\u001b[A\n",
            " 54% 93/172 [04:05<03:30,  2.67s/it]\u001b[A\n",
            " 55% 94/172 [04:08<03:27,  2.67s/it]\u001b[A\n",
            " 55% 95/172 [04:11<03:25,  2.67s/it]\u001b[A\n",
            " 56% 96/172 [04:13<03:22,  2.67s/it]\u001b[A\n",
            " 56% 97/172 [04:16<03:20,  2.67s/it]\u001b[A\n",
            " 57% 98/172 [04:19<03:17,  2.66s/it]\u001b[A\n",
            " 58% 99/172 [04:21<03:14,  2.66s/it]\u001b[A\n",
            " 58% 100/172 [04:24<03:11,  2.66s/it]\u001b[A\n",
            " 59% 101/172 [04:27<03:09,  2.66s/it]\u001b[A\n",
            " 59% 102/172 [04:29<03:06,  2.66s/it]\u001b[A\n",
            " 60% 103/172 [04:32<03:03,  2.66s/it]\u001b[A\n",
            " 60% 104/172 [04:35<03:00,  2.66s/it]\u001b[A\n",
            " 61% 105/172 [04:37<02:57,  2.66s/it]\u001b[A\n",
            " 62% 106/172 [04:40<02:55,  2.65s/it]\u001b[A\n",
            " 62% 107/172 [04:43<02:52,  2.66s/it]\u001b[A\n",
            " 63% 108/172 [04:45<02:50,  2.66s/it]\u001b[A\n",
            " 63% 109/172 [04:48<02:47,  2.66s/it]\u001b[A\n",
            " 64% 110/172 [04:51<02:44,  2.66s/it]\u001b[A\n",
            " 65% 111/172 [04:53<02:42,  2.66s/it]\u001b[A\n",
            " 65% 112/172 [04:56<02:39,  2.66s/it]\u001b[A\n",
            " 66% 113/172 [04:59<02:37,  2.66s/it]\u001b[A\n",
            " 66% 114/172 [05:01<02:34,  2.66s/it]\u001b[A\n",
            " 67% 115/172 [05:04<02:31,  2.66s/it]\u001b[A\n",
            " 67% 116/172 [05:06<02:28,  2.66s/it]\u001b[A\n",
            " 68% 117/172 [05:09<02:26,  2.66s/it]\u001b[A\n",
            " 69% 118/172 [05:12<02:23,  2.65s/it]\u001b[A\n",
            " 69% 119/172 [05:14<02:20,  2.66s/it]\u001b[A\n",
            " 70% 120/172 [05:17<02:18,  2.66s/it]\u001b[A\n",
            " 70% 121/172 [05:20<02:15,  2.66s/it]\u001b[A\n",
            " 71% 122/172 [05:22<02:12,  2.66s/it]\u001b[A\n",
            " 72% 123/172 [05:25<02:10,  2.66s/it]\u001b[A\n",
            " 72% 124/172 [05:28<02:07,  2.66s/it]\u001b[A\n",
            " 73% 125/172 [05:30<02:05,  2.66s/it]\u001b[A\n",
            " 73% 126/172 [05:33<02:02,  2.66s/it]\u001b[A\n",
            " 74% 127/172 [05:36<01:59,  2.66s/it]\u001b[A\n",
            " 74% 128/172 [05:38<01:56,  2.66s/it]\u001b[A\n",
            " 75% 129/172 [05:41<01:54,  2.66s/it]\u001b[A\n",
            " 76% 130/172 [05:44<01:51,  2.66s/it]\u001b[A\n",
            " 76% 131/172 [05:46<01:48,  2.66s/it]\u001b[A\n",
            " 77% 132/172 [05:49<01:46,  2.65s/it]\u001b[A\n",
            " 77% 133/172 [05:52<01:43,  2.65s/it]\u001b[A\n",
            " 78% 134/172 [05:54<01:40,  2.65s/it]\u001b[A\n",
            " 78% 135/172 [05:57<01:38,  2.65s/it]\u001b[A\n",
            " 79% 136/172 [06:00<01:35,  2.65s/it]\u001b[A\n",
            " 80% 137/172 [06:02<01:33,  2.66s/it]\u001b[A\n",
            " 80% 138/172 [06:05<01:30,  2.66s/it]\u001b[A\n",
            " 81% 139/172 [06:08<01:27,  2.66s/it]\u001b[A\n",
            " 81% 140/172 [06:10<01:24,  2.66s/it]\u001b[A\n",
            " 82% 141/172 [06:13<01:22,  2.66s/it]\u001b[A\n",
            " 83% 142/172 [06:16<01:19,  2.66s/it]\u001b[A\n",
            " 83% 143/172 [06:18<01:17,  2.66s/it]\u001b[A\n",
            " 84% 144/172 [06:21<01:14,  2.66s/it]\u001b[A\n",
            " 84% 145/172 [06:24<01:11,  2.66s/it]\u001b[A\n",
            " 85% 146/172 [06:26<01:09,  2.66s/it]\u001b[A\n",
            " 85% 147/172 [06:29<01:06,  2.66s/it]\u001b[A\n",
            " 86% 148/172 [06:32<01:03,  2.66s/it]\u001b[A\n",
            " 87% 149/172 [06:34<01:01,  2.66s/it]\u001b[A\n",
            " 87% 150/172 [06:37<00:58,  2.66s/it]\u001b[A\n",
            " 88% 151/172 [06:40<00:55,  2.66s/it]\u001b[A\n",
            " 88% 152/172 [06:42<00:53,  2.66s/it]\u001b[A\n",
            " 89% 153/172 [06:45<00:50,  2.66s/it]\u001b[A\n",
            " 90% 154/172 [06:48<00:47,  2.66s/it]\u001b[A\n",
            " 90% 155/172 [06:50<00:45,  2.67s/it]\u001b[A\n",
            " 91% 156/172 [06:53<00:42,  2.67s/it]\u001b[A\n",
            " 91% 157/172 [06:56<00:40,  2.67s/it]\u001b[A\n",
            " 92% 158/172 [06:58<00:37,  2.67s/it]\u001b[A\n",
            " 92% 159/172 [07:01<00:34,  2.67s/it]\u001b[A\n",
            " 93% 160/172 [07:04<00:31,  2.66s/it]\u001b[A\n",
            " 94% 161/172 [07:06<00:29,  2.66s/it]\u001b[A\n",
            " 94% 162/172 [07:09<00:26,  2.67s/it]\u001b[A\n",
            " 95% 163/172 [07:12<00:24,  2.67s/it]\u001b[A\n",
            " 95% 164/172 [07:14<00:21,  2.67s/it]\u001b[A\n",
            " 96% 165/172 [07:17<00:18,  2.67s/it]\u001b[A\n",
            " 97% 166/172 [07:20<00:16,  2.67s/it]\u001b[A\n",
            " 97% 167/172 [07:22<00:13,  2.67s/it]\u001b[A\n",
            " 98% 168/172 [07:25<00:10,  2.67s/it]\u001b[A\n",
            " 98% 169/172 [07:28<00:08,  2.67s/it]\u001b[A\n",
            " 99% 170/172 [07:30<00:05,  2.67s/it]\u001b[A\n",
            " 99% 171/172 [07:33<00:02,  2.67s/it]\u001b[A\n",
            "100% 172/172 [07:36<00:00,  2.66s/it]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 5.687814235687256, 'eval_runtime': 461.3263, 'eval_samples_per_second': 5.634, 'eval_steps_per_second': 5.634, 'epoch': 0.02}\n",
            "100% 25/25 [14:54<00:00, 16.75s/it]\n",
            "173it [07:39,  2.65s/it]\u001b[A\n",
            "                        \u001b[A/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "{'train_runtime': 904.3043, 'train_samples_per_second': 0.055, 'train_steps_per_second': 0.028, 'train_loss': 5.94542911529541, 'epoch': 0.02}\n",
            "100% 25/25 [15:04<00:00, 36.17s/it]\n",
            "[2024-10-29 16:58:20,505] [INFO] [axolotl.train.train:195] [PID:5495] [RANK:0] Training Completed!!! Saving pre-trained model to ./outputs/lora-out\u001b[39m\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict with trained model"
      ],
      "metadata": {
        "id": "NmeVpsC5IC9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch -m axolotl.cli.inference \"qlora-4bit_colab.yaml\" --qlora_model_dir=\"./outputs/lora-out\" --gradio"
      ],
      "metadata": {
        "id": "gxrob4RDJk-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f4362b-93c5-4e84-f797-c872ea3ff7f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2024-10-29 16:58:35.280172: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-29 16:58:35.299833: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-29 16:58:35.305767: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-29 16:58:35.322065: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-29 16:58:36.769792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/src/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:29: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, forward_function, hidden_states, *args):\n",
            "/content/src/axolotl/src/axolotl/utils/gradient_checkpointing/unsloth.py:40: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dY):\n",
            "[2024-10-29 16:58:42,291] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2024-10-29 16:58:42,431] [INFO] [root.spawn:60] [PID:10326] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmpggzrrvv2/test.c -o /tmp/tmpggzrrvv2/test.o\n",
            "[2024-10-29 16:58:42,481] [INFO] [root.spawn:60] [PID:10326] x86_64-linux-gnu-gcc /tmp/tmpggzrrvv2/test.o -laio -o /tmp/tmpggzrrvv2/a.out\n",
            "[2024-10-29 16:58:43,464] [INFO] [root.spawn:60] [PID:10326] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmp0v6jsxk7/test.c -o /tmp/tmp0v6jsxk7/test.o\n",
            "[2024-10-29 16:58:43,482] [INFO] [root.spawn:60] [PID:10326] x86_64-linux-gnu-gcc /tmp/tmp0v6jsxk7/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmp0v6jsxk7/a.out\n",
            "[2024-10-29 16:58:43,527] [INFO] [root.spawn:60] [PID:10326] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmp2aglyy0b/test.c -o /tmp/tmp2aglyy0b/test.o\n",
            "[2024-10-29 16:58:43,543] [INFO] [root.spawn:60] [PID:10326] x86_64-linux-gnu-gcc /tmp/tmp2aglyy0b/test.o -laio -o /tmp/tmp2aglyy0b/a.out\n",
            "/content/src/axolotl/src/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
            "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "\n",
            "     #@@ #@@      @@# @@#\n",
            "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
            "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
            "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
            "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
            "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
            "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
            "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
            "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
            "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
            "    @@@@  @@@@@@@@@@@@@@@@\n",
            "\n",
            "\u001b[33m[2024-10-29 16:58:45,383] [WARNING] [axolotl.utils.config.models.input.check_sample_packing_w_sdpa_bf16:1440] [PID:10326] [RANK:0] sample_packing & torch sdpa with bf16 is unsupported may results in 0.0 loss. This may work on H100s.\u001b[39m\n",
            "[2024-10-29 16:58:45,384] [DEBUG] [axolotl.normalize_config:83] [PID:10326] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
            "[2024-10-29 16:58:45,490] [INFO] [axolotl.normalize_config:207] [PID:10326] [RANK:0] GPU memory usage baseline: 0.000GB (+0.002GB cache, +0.458GB misc)\u001b[39m\n",
            "[2024-10-29 16:58:46,546] [INFO] [axolotl.common.cli.load_model_and_tokenizer:51] [PID:10326] [RANK:0] loading tokenizer... TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\u001b[39m\n",
            "[2024-10-29 16:58:47,023] [DEBUG] [axolotl.load_tokenizer:293] [PID:10326] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
            "[2024-10-29 16:58:47,023] [DEBUG] [axolotl.load_tokenizer:294] [PID:10326] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2024-10-29 16:58:47,023] [DEBUG] [axolotl.load_tokenizer:295] [PID:10326] [RANK:0] PAD: 32000 / <|end_of_text|>\u001b[39m\n",
            "[2024-10-29 16:58:47,023] [DEBUG] [axolotl.load_tokenizer:296] [PID:10326] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2024-10-29 16:58:47,023] [INFO] [axolotl.load_tokenizer:310] [PID:10326] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2024-10-29 16:58:47,023] [INFO] [axolotl.common.cli.load_model_and_tokenizer:53] [PID:10326] [RANK:0] loading model and (optionally) peft_config...\u001b[39m\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
            "[2024-10-29 16:59:09,587] [INFO] [axolotl.load_model:1059] [PID:10326] [RANK:0] GPU memory usage after model load: 0.841GB (+0.043GB cache, +0.474GB misc)\u001b[39m\n",
            "[2024-10-29 16:59:09,623] [INFO] [axolotl.load_model:1087] [PID:10326] [RANK:0] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
            "[2024-10-29 16:59:09,627] [INFO] [axolotl.load_lora:1284] [PID:10326] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
            "trainable params: 156,307,456 || all params: 1,256,359,936 || trainable%: 12.4413\n",
            "[2024-10-29 16:59:10,065] [INFO] [axolotl.load_model:1156] [PID:10326] [RANK:0] GPU memory usage after adapters: 1.426GB (+0.047GB cache, +0.474GB misc)\u001b[39m\n",
            "IMPORTANT: You are using gradio version 3.50.2, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://d51eab0863fb2f9f4c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1209, in wait\n",
            "Keyboard interruption in main thread... closing server.\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1168, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 760, in simple_launcher\n",
            "    process.wait()\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1222, in wait\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1953, in _wait\n",
            "    time.sleep(delay)\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://d51eab0863fb2f9f4c.gradio.live\n",
            "\u001b[0mterminate called without an active exception\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deeper Dive"
      ],
      "metadata": {
        "id": "6qQyj7CyG2LK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is also helpful to gain some familiarity over some of the core inner workings of axolotl"
      ],
      "metadata": {
        "id": "2cigiyK_G4cR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Configuration Normalization"
      ],
      "metadata": {
        "id": "oa4YfymFu4_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Axolotl uses a custom Dict class, called ```DictDefault```\n",
        "to store configurations specified in the yaml configuration file (into a Python variable named ```cfg```). The definition for this custom Dict can be found in the [utils/dict.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/utils/dict.py)\n",
        "\n",
        "```DictDefault``` is amended such that calling a missing key from it will result in a ```None``` return type. This is important because if some configuration options aren't specified by the user, the '''None''' type allows Axolotl to perform boolean operations to determine the default settings for missing configurations. For more examples on how this is done, check out [utils/config/__init__.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/utils/config/__init__.py)"
      ],
      "metadata": {
        "id": "tQijYwJAK5HO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading Models, Tokenizers, and Trainer"
      ],
      "metadata": {
        "id": "fmSTsXPLNdk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we inspect [cli.train.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/cli/train.py), we will find that most of the heavy lifting were done by the function ```train()``` which is itself imported from [src/axolotl/train.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/train.py).\n",
        "\n",
        "```train()``` takes care of loading the appropriate tokenizer and pre-trained model through ```load_model()``` and ```load_tokenizer()``` from [src/axolotl/utils/models.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/utils/models.py) respectively.\n",
        "\n",
        "```load_tokenizer()``` loads in the appropriate tokenizer given the desired model, as well as chat templates.\n",
        "\n",
        "```ModelLoader``` class follows after tokenizer has been selected. It will automatically discern the base model type, load in the desired model, as well as applying model-appropriate attention mechanism modifications (e.g. flash attention). Depending on which base model the user chooses in the configuration, ```ModelLoader``` will utilize the corresponding \"attention hijacking\" script. For example, if the user specified the base model to be ```NousResearch/Meta-Llama-3.1-8B```, which is of llama type, and set ```flash_attn``` to ```True```, ```ModelLoader``` will load in [llama_attn_hijack_flash.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/monkeypatch/llama_attn_hijack_flash.py). For a list of supported attention hijacking, please refer to the directory [/src/axolotl/monkeypatch/](https://github.com/axolotl-ai-cloud/axolotl/tree/main/src/axolotl/monkeypatch)\n",
        "\n",
        "Another important operation encompassed in ```train()``` is setting up the training that takes into account of user-specified traning configurations (e.g. num_epochs, optimizer) through the use of ```setup_trainer()``` from [/src/axolotl/utils/trainer.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/utils/trainer.py), which in turn relies on modules from [/src/axolotl/core/trainer_builder.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/core/trainer_builder.py).\n",
        "```trainer_builder.py``` provides a list of trainer object options bespoke for the task type (Causal or Reinforcement learning ('dpo', 'ipo', 'kto') )"
      ],
      "metadata": {
        "id": "GX0Y0HSsRevo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Monkey patch\n",
        "\n",
        "The [Monkey patch directory](https://github.com/axolotl-ai-cloud/axolotl/tree/main/src/axolotl/monkeypatch) is where model architecture/optimization patching scripts are stored (these are modifications that are not implemented in the official releases, hence the name monkey patch). It includes attention jacking, ReLoRA, and unsloth optimization.\n"
      ],
      "metadata": {
        "id": "k_asIYgcJ_LZ"
      }
    }
  ]
}