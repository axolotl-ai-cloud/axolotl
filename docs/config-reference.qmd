---
title: Config Reference
description: A complete list of all configuration options.
---

```yaml
# Allow overwrite yml config using from cli
strict: boolean | None = False
# Resume from a specific checkpoint dir
resume_from_checkpoint: string | None
# If resume_from_checkpoint isn't set and you simply want it to start where it left off.
# Be careful with this being turned on between different models.
auto_resume_from_checkpoints: boolean | None
# Resize the model embeddings when new tokens are added to multiples of 32. This is
# reported to improve training speed on some models
resize_token_embeddings_to_32x: boolean | None
mean_resizing_embeddings: boolean | None = False

# Whether to shrink the embeddings to len(tokenizer). By default, we won't shrink.
shrink_embeddings: boolean | None
# Don't upcast the embeddings to float32 when using PEFT. Useful for low-VRAM GPUs
embeddings_skip_upcast: boolean | None

# Use RL training: 'dpo', 'ipo', 'kto', 'simpo', 'orpo', 'grpo'
rl: RLType | None
trl: TRLConfig | None
vllm: VllmConfig | None
# Reward modelling: `True` or `False`
reward_model: boolean | None
# Process reward modelling: `True` or `False`
process_reward_model: boolean | None
num_labels: integer | None

# Whether to perform weighting in DPO trainer
dpo_use_weighting: boolean | None
dpo_use_logits_to_keep: boolean | None

# A list of one or more datasets to finetune the model with
datasets: array | None

# A list of one or more datasets to eval the model with. You can use either
# test_datasets, or val_set_size, but not both.
test_datasets: array | None
# If false, the datasets will not be shuffled and will keep their original order in
# `datasets`. The same applies to the `test_datasets` option and the
# `pretraining_dataset` option. Default is true.
shuffle_merged_datasets: boolean | None = True
# Axolotl attempts to save the dataset as an arrow after packing the data together so
# subsequent training attempts load faster, relative path
dataset_prepared_path: string | None
# Num shards for whole dataset
dataset_shard_num: integer | None
# Index of shard to use for whole dataset
dataset_shard_idx: integer | None
skip_prepare_dataset: boolean | None = False

# Set to HF dataset for type: 'completion' for streaming instead of pre-tokenize
pretraining_dataset: array | None
# The maximum number of processes to use while preprocessing your input dataset. This
# defaults to `os.cpu_count()` if not set.
dataset_processes: integer | None = 14
# Deduplicates datasets and test_datasets with identical entries
dataset_exact_deduplication: boolean | None
# Keep dataset in memory while preprocessing. Only needed if cached dataset is taking
# too much storage
dataset_keep_in_memory: boolean | None
dataloader_pin_memory: boolean | None
dataloader_num_workers: integer | None
dataloader_prefetch_factor: integer | None
dataloader_drop_last: boolean | None

accelerator_config: object | None

remove_unused_columns: boolean | None

# Push prepared dataset to hub - repo_org/repo_name
push_dataset_to_hub: string | None
# Whether to use hf `use_auth_token` for loading datasets. Useful for fetching private
# datasets. Required to be true when used in combination with `push_dataset_to_hub`
hf_use_auth_token: boolean | None

device: unknown | None
# Passed through to transformers when loading the model when launched without
# accelerate. Use `sequential` when training w/ model parallelism to limit memory
device_map: unknown | None
world_size: integer | None
# Don't mess with this, it's here for accelerate and torchrun
local_rank: integer | None
ddp: boolean | None

# Seed for reproducibility
seed: integer | None
# Advanced DDP Arguments - timeout
ddp_timeout: integer | None
# Advanced DDP Arguments - bucket cap in MB
ddp_bucket_cap_mb: integer | None
# Advanced DDP Arguments - broadcast buffers
ddp_broadcast_buffers: boolean | None
ddp_find_unused_parameters: boolean | None

# Approximate number of predictions sent to wandb depending on batch size. Enabled above
# 0. Default is 0
eval_table_size: integer | None
# Total number of tokens generated for predictions sent to wandb. Default is 128
eval_max_new_tokens: integer | None
# Whether to run causal language model evaluation for metrics in
# `eval_causal_lm_metrics`
do_causal_lm_eval: boolean | None
# HF evaluate metrics used during evaluation. Default is ['sacrebleu', 'comet', 'ter',
# 'chrf', 'perplexity']
eval_causal_lm_metrics: array | None
do_bench_eval: boolean | None
bench_dataset: string | None
bench_split: string | None
metric_for_best_model: string | None
greater_is_better: boolean | None

# High loss value, indicating the learning has broken down (a good estimate is ~2 times
# the loss at the start of training)
loss_watchdog_threshold: number | None
# Number of high-loss steps in a row before the trainer aborts (default: 3)
loss_watchdog_patience: integer | None

gc_steps: integer | None

# Use CUDA bf16. bool or 'full' for `bf16_full_eval`, or 'auto' for automatic detection.
# require >=ampere
bf16: string | boolean | None = auto
# Use CUDA fp16
fp16: boolean | None
fp8: boolean | None
# No AMP (automatic mixed precision) - require >=ampere
bfloat16: boolean | None
# No AMP (automatic mixed precision)
float16: boolean | None
# Use CUDA tf32 - require >=ampere
tf32: boolean | None
float32: boolean | None

# Whether to use gradient checkpointing. Available options are: true, false, 'offload',
# 'offload_disk'.
# https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-checkpointing
gradient_checkpointing: string | boolean | None = False
# Additional kwargs to pass to the trainer for gradient checkpointing
gradient_checkpointing_kwargs: object | None

unfrozen_parameters: array | None

# The maximum length of an input to train with, this should typically be less than 2048
# as most models have a token/context limit of 2048
sequence_len: integer = 512
min_sample_len: integer | None
# maximum prompt length for RL training
max_prompt_len: integer = 512
# Use efficient multi-packing with block diagonal attention and per sequence
# position_ids. Recommend set to 'true'
sample_packing: boolean | None
# The number of samples packed at a time. Increasing the following values helps with
# packing, but usually only slightly (<%1.)
sample_packing_group_size: integer | None = 100000
# The number of samples which can be packed into one sequence. Increase if using a large
# sequence_len with many short samples.
sample_packing_bin_size: integer | None = 200
# Whether to pack samples sequentially
sample_packing_sequentially: boolean | None
# Set to 'false' if getting errors during eval with sample_packing on
eval_sample_packing: boolean | None
# Pad inputs so each step uses constant sized buffers. This will reduce memory
# fragmentation and may prevent OOMs, by re-using memory more efficiently
pad_to_sequence_len: boolean | None
# Whether to use sequential sampling for curriculum learning
curriculum_sampling: boolean | None
multipack_real_batches: boolean | None
# whether to concatenate samples during pretraining
pretraining_sample_concatenation: boolean | None

# Use batch flattening for speedups when not using sample_packing
batch_flattening: string | boolean | None

use_pose: boolean | None
pose_split_on_token_ids: array | None
pose_max_context_len: integer | None
pose_num_chunks: integer | None

pretrain_multipack_buffer_size: integer | None = 10000
# whether to prevent cross attention for packed sequences during pretraining
pretrain_multipack_attn: boolean | None = True

# Whether to use xformers attention patch https://github.com/facebookresearch/xformers
xformers_attention: boolean | None
# Whether to use scaled-dot-product attention https://pytorch.org/docs/stable/generated/
# torch.nn.functional.scaled_dot_product_attention.html
sdp_attention: boolean | None
# Shifted-sparse attention (only llama) - https://arxiv.org/pdf/2309.12307.pdf
s2_attention: boolean | None
flex_attention: boolean | None
flex_attn_compile_kwargs: object | None
# Whether to use flash attention patch https://github.com/Dao-AILab/flash-attention
flash_attention: boolean | None
# Whether to use flash-attention cross entropy implementation - advanced use only
flash_attn_cross_entropy: boolean | None
# Whether to use flash-attention rms norm implementation - advanced use only
flash_attn_rms_norm: boolean | None
# Whether to fuse QKV into a single operation
flash_attn_fuse_qkv: boolean | None
# Whether to fuse part of the MLP into a single operation
flash_attn_fuse_mlp: boolean | None
# Whether to use bettertransformers
flash_optimum: boolean | None

eager_attention: boolean | None

unsloth_cross_entropy_loss: boolean | None
unsloth_lora_mlp: boolean | None
unsloth_lora_qkv: boolean | None
unsloth_lora_o: boolean | None
unsloth_rms_norm: boolean | None
unsloth_rope: boolean | None

# Apply custom LoRA autograd functions and activation function Triton kernels for speed
# and memory savings. See: https://docs.axolotl.ai/docs/lora_optims.html
lora_mlp_kernel: boolean | None
# Apply custom LoRA autograd functions and activation function Triton kernels for speed
# and memory savings. See: https://docs.axolotl.ai/docs/lora_optims.html
lora_qkv_kernel: boolean | None
# Apply custom LoRA autograd functions and activation function Triton kernels for speed
# and memory savings. See: https://docs.axolotl.ai/docs/lora_optims.html
lora_o_kernel: boolean | None

llama4_linearized_experts: boolean | None

# Deepspeed config path. e.g., deepspeed_configs/zero3.json
deepspeed: string | object | None
# FSDP configuration
fsdp: array | None
# FSDP configuration options
fsdp_config: object | None
fsdp_final_state_dict_type: string | None

# How much of the dataset to set aside as evaluation. 1 = 100%, 0.50 = 50%, etc. 0 for
# no eval.
val_set_size: number | None = 0.0

# Set to a divisor of the number of GPUs available to split sequences into chunks of
# equal size. Use in long context training to prevent OOM when sequences cannot fit into
# a single GPU's VRAM. E.g., if 4 GPUs are available, set this value to 2 to split each
# sequence into two equal-sized subsequences, or set to 4 to split into four equal-sized
# subsequences. See https://docs.axolotl.ai/docs/sequence_parallelism.html for more
# details.
sequence_parallel_degree: integer | None
# Optional; strides across the key dimension. Larger values use more memory but should
# make training faster. Must evenly divide the number of KV heads in your model.
heads_k_stride: integer | None
# One of 'varlen_llama3', 'batch_ring', 'batch_zigzag', 'batch_stripe'. Defaults to
# 'varlen_llama3' in the sample packing case, and 'batch_ring' in the non-sample packing
# case.
ring_attn_func: RingAttnFunc | None

# Add or change special tokens. If you add tokens here, you don't need to add them to
# the `tokens` list.
special_tokens: SpecialTokensConfig | None
# Add extra tokens to the tokenizer
tokens: array | None
# Mapping token_id to new_token_string to override reserved added_tokens in the
# tokenizer. Only works for tokens that are not part of the base vocab (aka are
# added_tokens). Can be checked if they exist in tokenizer.json added_tokens.
added_tokens_overrides: object | None

# Whether to use torch.compile and which backend to use. setting to `auto` will enable
# torch compile when torch>=2.5.1
torch_compile: string | boolean | None
# Backend to use for torch.compile
torch_compile_backend: string | None
torch_compile_mode: string | None

# Maximum number of iterations to train for. It precedes num_epochs which means that if
# both are set, num_epochs will not be guaranteed. e.g., when 1 epoch is 1000 steps =>
# `num_epochs: 2` and `max_steps: 100` will train for 100 steps
max_steps: integer | None
# Number of warmup steps. Cannot use with warmup_ratio
warmup_steps: integer | None
# Warmup ratio. Cannot use with warmup_steps
warmup_ratio: number | None
# Leave empty to eval at each epoch, integer for every N steps. float for fraction of
# total steps
eval_steps: integer | number | None
# Number of times per epoch to run evals, mutually exclusive with eval_steps
evals_per_epoch: integer | None
# Set to `no` to skip evaluation, `epoch` at end of each epoch, leave empty to infer
# from `eval_steps`
eval_strategy: string | None
# Leave empty to save at each epoch, integer for every N steps. float for fraction of
# total steps
save_steps: integer | number | None
# Number of times per epoch to save a checkpoint, mutually exclusive with save_steps
saves_per_epoch: integer | None
# Set to `no` to skip checkpoint saves, `epoch` at end of each epoch, `best` when better
# result is achieved, leave empty to infer from `save_steps`
save_strategy: string | None
# Checkpoints saved at a time
save_total_limit: integer | None
# Logging frequency
logging_steps: integer | None
# Stop training after this many evaluation losses have increased in a row. https://huggi
# ngface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppin
# gCallback
early_stopping_patience: integer | None
load_best_model_at_end: boolean | None = False
# Save only the model weights, skipping the optimizer. Using this means you can't resume
# from checkpoints.
save_only_model: boolean | None = False
# Use tensorboard for logging
use_tensorboard: boolean | None
# Enable the pytorch profiler to capture the first N steps of training to the
# output_dir. see https://pytorch.org/blog/understanding-gpu-memory-1/ for more
# information. Snapshots can be visualized @ https://pytorch.org/memory_viz
profiler_steps: integer | None
# bool of whether to include tokens trainer per second in the training metrics. This
# iterates over the entire dataset once, so it takes some time.
include_tokens_per_second: boolean | None

# NEFT https://arxiv.org/abs/2310.05914, set this to a number (paper default is 5) to
# add noise to embeddings. Currently only supported on Llama and Mistral
neftune_noise_alpha: number | None

# Parameter controlling the relative ratio loss weight in the ORPO loss. Passed to
# `beta` in `ORPOConfig` due to trl mapping.
orpo_alpha: number | None
# Weighting of NLL term in loss from RPO paper
rpo_alpha: number | None
# Target reward margin for the SimPO loss
simpo_gamma: number | None
# Weight of the BC regularizer
cpo_alpha: number | None

# Factor for desirable loss term in KTO loss
kto_desirable_weight: number | None
# Factor for undesirable loss term in KTO loss
kto_undesirable_weight: number | None
# The beta parameter for the RL training
rl_beta: number | None

# Defines the max memory usage per gpu on the system. Passed through to transformers
# when loading the model.
max_memory: object | None
# Limit the memory for all available GPUs to this amount (if an integer, expressed in
# gigabytes); default: unset
gpu_memory_limit: integer | string | None
# Whether to use low_cpu_mem_usage
low_cpu_mem_usage: boolean | None

# The name of the chat template to use for training, following values are supported: -
# tokenizer_default: Uses the chat template that is available in the
# tokenizer_config.json. If the chat template is not available in the tokenizer, it will
# raise an error. This is the default value. -
# alpaca/inst/chatml/gemma/cohere/llama3/phi_3/deepseek_v2/jamba: These chat templates
# are available in the axolotl codebase at src/axolotl/utils/chat_templates.py -
# tokenizer_default_fallback_*: where * is the name of the chat template to fallback to.
# E.g. tokenizer_default_fallback_chatml. This is useful when the chat template is not
# available in the tokenizer. - jinja: Uses a custom jinja template for the chat
# template. The custom jinja template should be provided in the chat_template_jinja
# field. The selected chat template will be saved to the tokenizer_config.json for
# easier inferencing
chat_template: ChatTemplate | string | None
# Custom jinja template for chat template. This will be only used if chat_template is
# set to `jinja` or `null` (in which case chat_template is automatically set to
# `jinja`). Default is null.
chat_template_jinja: string | None
# Custom EOT (End-of-Turn) tokens to mask/unmask during training. These tokens mark the
# boundaries between conversation turns. For example: ['/INST', '</s>',
# '[/SYSTEM_PROMPT]']. If not specified, defaults to just the model's eos_token. This is
# useful for templates that use multiple delimiter tokens.
eot_tokens: array | None
# Changes the default system message. Currently only supports chatml.
default_system_message: string | None

fix_untrained_tokens: integer | array | None

is_preprocess: boolean | None
preprocess_iterable: boolean | None

# Total number of tokens - internal use
total_num_tokens: integer | None
total_supervised_tokens: integer | None
# You can set these packing optimizations AFTER starting a training at least once. The
# trainer will provide recommended values for these values.
sample_packing_eff_est: number | None
axolotl_config_path: string | None

# Internal use only - Used to identify which the model is based on
is_falcon_derived_model: boolean | None
# Internal use only - Used to identify which the model is based on
is_llama_derived_model: boolean | None
# Internal use only - Used to identify which the model is based on. Please note that if
# you set this to true, `padding_side` will be set to 'left' by default
is_mistral_derived_model: boolean | None
# Internal use only - Used to identify which the model is based on
is_qwen_derived_model: boolean | None

# Add plugins to extend the pipeline. See `src/axolotl/integrations` for the available
# plugins or doc below for more details.
# https://docs.axolotl.ai/docs/custom_integrations.html
plugins: array | None
```